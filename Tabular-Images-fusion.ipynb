{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import pandas as pd\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pickle as pkl\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "from transformers import Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 346M/346M [06:37<00:00, 870kB/s] \n"
     ]
    }
   ],
   "source": [
    "#Prepare image model \n",
    "import glob\n",
    "from PIL import Image\n",
    "vit_extractor = ViTImageProcessor()\n",
    "#print(vit_extractor)\n",
    "#dataset = load_dataset(\"./Maps-snapshots\")\n",
    "image_list = []\n",
    "for filename in glob.glob('./Maps-snapshots/*.jpeg'):\n",
    "    im=Image.open(filename)\n",
    "    image_list.append(im)\n",
    "\n",
    "image = image_list[0]\n",
    "model_inputs  = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m label_list \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mBAD\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGOOD\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# what each label class represents\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m torch_dataset \u001b[39m=\u001b[39m load_data(\n\u001b[0;32m     16\u001b[0m     data_df,\n\u001b[0;32m     17\u001b[0m     text_cols,\n\u001b[0;32m     18\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[0;32m     19\u001b[0m     categorical_cols\u001b[39m=\u001b[39;49mcategorical_cols,\n\u001b[0;32m     20\u001b[0m     numerical_cols\u001b[39m=\u001b[39;49mnumerical_cols,\n\u001b[0;32m     21\u001b[0m     label_col \u001b[39m=\u001b[39;49m label_col\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m num_labels \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(torch_dataset, labels))\n\u001b[0;32m     24\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rhanizar\\anaconda3\\envs\\multimodal\\lib\\site-packages\\multimodal_transformers\\data\\load_data.py:386\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(data_df, text_cols, tokenizer, label_col, label_list, categorical_cols, numerical_cols, sep_text_token_str, categorical_encode_type, numerical_transformer, empty_text_values, replace_empty_text, max_token_length, debug, debug_dataset_size)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m empty_text_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     empty_text_values \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mnan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 386\u001b[0m text_cols_func \u001b[39m=\u001b[39m convert_to_func(text_cols)\n\u001b[0;32m    387\u001b[0m categorical_cols_func \u001b[39m=\u001b[39m convert_to_func(categorical_cols)\n\u001b[0;32m    388\u001b[0m numerical_cols_func \u001b[39m=\u001b[39m convert_to_func(numerical_cols)\n",
      "File \u001b[1;32mc:\\Users\\rhanizar\\anaconda3\\envs\\multimodal\\lib\\site-packages\\multimodal_transformers\\data\\data_utils.py:122\u001b[0m, in \u001b[0;36mconvert_to_func\u001b[1;34m(container_arg)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m df, x: \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(container_arg, types\u001b[39m.\u001b[39mFunctionType):\n\u001b[1;32m--> 122\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(container_arg) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mtype\u001b[39m(container_arg) \u001b[39mis\u001b[39;00m \u001b[39mset\u001b[39m\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m df, x: x \u001b[39min\u001b[39;00m container_arg\n\u001b[0;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Prepare tabular model \n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import pandas as pd\n",
    "from multimodal_transformers.data import load_data\n",
    "from multimodal_transformers.model import AutoModelWithTabular, TabularConfig\n",
    "\n",
    "data_df = pd.read_csv('Dataset/DATASET1304.csv')\n",
    "text_cols = ['TEXTE', 'TEXTE2']\n",
    "categorical_cols = [ 'LANES','TYPE TRAIT', 'HAS SHOULDER', 'P_PENETRANTE', 'P_PEAGE', 'P_AIRE_SERVICE', 'ENTREE VILLE']\n",
    "numerical_cols = ['TMJA', 'TR', 'DMU']\n",
    "label_col = 'SCORE'\n",
    "label_list = ['BAD', 'GOOD'] # what each label class represents\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "torch_dataset = load_data(\n",
    "    data_df,\n",
    "    text_cols,\n",
    "    tokenizer=tokenizer,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numerical_cols=numerical_cols,\n",
    "    label_col = label_col\n",
    ")\n",
    "num_labels = len(np.unique(torch_dataset, labels))\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "tabular_config = TabularConfig(\n",
    "    num_labels=num_labels,\n",
    "    cat_feat_dim=torch_dataset.cat_feats.shape[1],\n",
    "    numerical_feat_dim=torch_dataset.numerical_feats.shape[1],\n",
    "    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n",
    ")\n",
    "config.tabular_config = tabular_config\n",
    "\n",
    "model = AutoModelWithTabular.from_pretrained('bert-base-uncased', config=config)\n",
    "# tabular_config = TabularConfig(\n",
    "#     num_labels=3,\n",
    "#     cat_feat_dim=categorical_cols.count,\n",
    "#     numerical_feat_dim=numerical_cols.count,\n",
    "#     combine_feat_method='attention_on_cat_and_numerical_feats',\n",
    "# )\n",
    "# config.tabular_config = tabular_config\n",
    "\n",
    "# tabularModel = AutoModelWithTabular.from_pretrained('ipuneetrathore/bert-base-cased-finetuned-finBERT', config=config)\n",
    "\n",
    "# model_inputs['cat_feats'] = categorical_cols\n",
    "# model_inputs['num_feats'] = numerical_cols\n",
    "# model_inputs['labels'] = label_cols \n",
    "# loss, logits, layer_outs = tabularModel(**model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multimodal processing \n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
